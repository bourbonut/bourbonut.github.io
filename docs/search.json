[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Introduction to Reinforcement Learning\n\n\n\nartificial intelligence\n\n\n\n\n\n\n\nBenjamin Bourbon\n\n\nJan 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Federated Learning\n\n\n\nartificial intelligence\n\n\n\n\n\n\n\nBenjamin Bourbon\n\n\nJan 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Spiking Neural Networks\n\n\n\nartificial intelligence\n\n\n\n\n\n\n\nBenjamin Bourbon\n\n\nJan 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSpur Tooth Profile\n\n\n\nmechanic\n\n\n\n\n\n\n\nBenjamin Bourbon\n\n\nJan 8, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Benjamin Bourbon",
    "section": "",
    "text": "I’m a young engineer with a specialization in High Performance Computing and Artificial Intelligence.\nI develop currently a CAD library pymadcad with a friend.\nThis blog aims to share some personal work which I find useful."
  },
  {
    "objectID": "posts/involute-spur-gear/index.html",
    "href": "posts/involute-spur-gear/index.html",
    "title": "Spur Tooth Profile",
    "section": "",
    "text": "Involute gears are the most popular power transmission devices. We find them as an essential component for many machines. For instance, epicyclic gear train is a combination of gears which allows to make a reduction in a compact way. Spur gears are the most popular form and the most efficient type of gearing."
  },
  {
    "objectID": "posts/involute-spur-gear/index.html#code",
    "href": "posts/involute-spur-gear/index.html#code",
    "title": "Spur Tooth Profile",
    "section": "Code",
    "text": "Code\nfrom collections import namedtuple\nfrom functools import partial\nfrom manim import * # pip install manim\n\ndef profile(m, z, alpha=radians(20), ka=1, kf=1.25, interference=True):\n    # Parameters\n    ha = m * ka  #                  addendum height\n    hf = m * kf  #                  dedendum height\n    p = pi * m  #                   step\n    rp = pitch_radius(m, z) #       pitch radius\n    ra = rp + ha #                  addendum radius\n    rf = rp - hf #                  dedendum radius\n    rb = base_radius(m, z, alpha) # base radius\n\n    ta = angle_involute(ra, rb) #   addendum angle\n    tp = angle_involute(rp, rb) #   pitch angle\n\n    duplicate = (\n        lambda obj, angle: obj.copy()\n        .apply_matrix(mat3(X, -Y, Z))\n        .rotate_about_origin(angle)\n    )\n    phase = pi / z + 2 * (tp - atan2(tp, 1))\n\n    # Involute\n    side = ParametricFunction(partial(involute, r=rb), t_range=[0, ta])\n\n    # Arc parameters\n    ArcParameters = namedtuple(\"ArcParameters\", [\"center\", \"radius\", \"angle\"])\n    r = 0.5 * (rb - rf)\n    t = -atan2(r, rf + r)\n    arcp = ArcParameters((rf + r) * u(t), r, t)\n    arc = Arc(arcp.radius, -pi + arcp.angle, -pi / 2, arc_center=arcp.center)\n\n    # Joint, top and bottom\n    angle_top = ta - atan2(ta, 1)\n    top = Arc(ra, angle_top, phase - 2 * angle_top)\n    joint = Line(arcp.center + arcp.radius * u(-3 * pi / 2 + arcp.angle), rb * X)\n    M = arcp.center + arcp.radius * u(-pi + arcp.angle)\n    angle_bottom = anglebt(M, u(0.5 * phase)) * 2\n    bottom = Line(M, rotation(-2 * pi / z + angle_bottom) * M)\n\n    # Patches\n    top_dot = Dot(ra * u(angle_top), radius=0.02)\n    side_dot = Dot(rb * X, radius=0.02)\n    point = arcp.center + arcp.radius * u(-3 * pi / 2 + arcp.angle)\n    joint_dot = Dot(point, radius=0.02)\n    bottom_dot = Dot(M, radius=0.02)\n    dots = (top_dot, side_dot, joint_dot, bottom_dot)\n\n    # Duplicated objects\n    duplicated_objs = map(\n        partial(duplicate, angle=phase), (side, arc, joint) + dots\n    )\n\n    return VGroup(\n        side, top, arc, joint, bottom, *duplicated_objs, *dots\n    ).rotate_about_origin(-phase * 0.5)\nNote : for manim, see installation"
  },
  {
    "objectID": "posts/involute-spur-gear/index.html#code-1",
    "href": "posts/involute-spur-gear/index.html#code-1",
    "title": "Spur Tooth Profile",
    "section": "Code",
    "text": "Code\nfrom collections import namedtuple\nfrom functools import partial\nfrom manim import *\n\ndef involute(t, r, t0=0):\n    return r * (u(t - t0) - t * v(t - t0))\n\n\ndef interference_curve(t, r, x, y, t0):\n    return involute(t, r, t0) - x * u(t - t0) + y * v(t - t0)\n\n\ndef derived_involute(t, r, t0):\n    return r * t * u(t - t0)\n\n\ndef derived_interference_curve(t, r, x, y, t0):\n    return derived_involute(t, r, t0) - x * v(t - t0) - y * u(t - t0)\n\n\ndef jacobian_involute(rb, rp, x, y, t0):\n    return lambda t1, t2: mat3(\n        derived_involute(t1, rb, t0), \n        -derived_interference_curve(t2, rp, x, y, t0),\n        Z,\n    )\n\ndef angle_involute(r, rb):\n    return sqrt(r * r / (rb * rb) - 1)\n\ndef profile(m, z, alpha=radians(20), ka=1, kf=1.25, interference=True):\n    # Parameters\n    ha = m * ka  #                  addendum height\n    hf = m * kf  #                  dedendum height\n    p = pi * m  #                   step\n    rp = pitch_radius(m, z) #       pitch radius\n    ra = rp + ha #                  addendum radius\n    rf = rp - hf #                  dedendum radius\n    rb = base_radius(m, z, alpha) # base radius\n\n    ta = angle_involute(ra, rb) #   addendum angle\n    tp = angle_involute(rp, rb) #   pitch angle\n\n    duplicate = (\n        lambda obj, angle: obj.copy()\n        .apply_matrix(mat3(X, -Y, Z))\n        .rotate_about_origin(angle)\n    )\n\n    la = rack.addendum_length(m, alpha, ka)\n    ts = tp - atan2(tp, 1)\n    phase = pi / z + 2 * (tp - atan2(tp, 1))\n    phase_empty = 2 * pi / z - phase\n    angle_top = ta - atan2(ta, 1)\n    tmin = la * 0.5 / rp\n\n    Functions = namedtuple(\"Functions\", [\"involute\", \"interference\"])\n    functions = Functions(\n        partial(involute, r=rb),\n        partial(interference_curve, r=rp, x=ha, y=0.5 * la, t0=phase_empty * 0.5),\n    )\n\n    # Newton method\n    f = lambda t1, t2: functions.involute(t1) - functions.interference(t2)\n    J = jacobian_involute(rb, rp, ha, 0.5 * la, phase_empty * 0.5)\n    # t3 is not used, but exists because 3D vectors\n    t1, t2, t3 = 0.5 * ta, -0.5 * ta, 0\n    for i in range(8):\n        t1, t2, t3 = vec3(t1, t2, t3) - inverse(J(t1, t2)) * f(t1, t2)\n\n    # Involute and interference curve\n    side = ParametricFunction(functions.involute, t_range=[t1, ta])\n    interference = ParametricFunction(functions.interference, t_range=[t2, tmin])\n\n    # Top and bottom\n    top = Arc(ra, angle_top, phase - 2 * angle_top)\n    M = functions.interference(tmin)\n    angle_bottom = anglebt(M, u(0.5 * phase)) * 2\n    bottom = Line(M, rotation(-2 * pi / z + angle_bottom) * M)\n\n    # Patches\n    top_dot = Dot(ra * u(angle_top), radius=0.02)\n    interference_dot = Dot(functions.interference(t2), radius=0.02)\n    involute_dot = Dot(functions.involute(t1), radius=0.02)\n    bottom_dot = Dot(M, radius=0.02)\n    dots = (top_dot, interference_dot, involute_dot, bottom_dot)\n\n    # Duplicated objects\n    duplicated_objs = map(\n        partial(duplicate, angle=phase), (side, interference) + dots\n    )\n\n    return VGroup(\n        side, interference, *duplicated_objs, top, bottom, *dots\n    ).rotate_about_origin(-phase * 0.5)\nAnd the gear profile :\n\n\n\nFigure : Gear profile with interference\n\n\nNote the closest radius to the pinion’s center is greater than the dedendum radius due to the interference curve.\nLet’s see the rack movement now :\n\n\nVideo\nAnimation : Rack moving around the pinion"
  },
  {
    "objectID": "posts/introduction-reinforcement-learning/index.html",
    "href": "posts/introduction-reinforcement-learning/index.html",
    "title": "Introduction to Reinforcement Learning",
    "section": "",
    "text": "Reinforcement Learning (RL) is a branch of Artificial Intelligence where an agent (the model) and the environment are the basic components of it. The agent interacts with the environment by taking actions and receives rewards according to the impact of the action taken. The objective of the agent is to learn to behave in such a way as to maximize the expected rewards in the long term."
  },
  {
    "objectID": "posts/introduction-reinforcement-learning/index.html#example",
    "href": "posts/introduction-reinforcement-learning/index.html#example",
    "title": "Introduction to Reinforcement Learning",
    "section": "Example",
    "text": "Example\nImagine a red dot which can move into six directions : \\(X, Y, Z, -X, -Y, -Z\\). The agent is defined as the red dot which is more precisely a model such as a deep neural network. The directions are called actions and the agent can move each step by choosing one of its actions. The environment is defined as a 3D space where the agent has a position \\((x, y, z)\\). The state of the environment is the position of the agent. \n\nGoal\nThe goal of the agent (red dot) is to find the shortest path to eat the blue dot. In this example, there is no rule to restart the experience except by waiting until the agent has successfully reached the blue dot. Therefore, a maximum of attempts of actions is defined and is higher than the minimum number of actions needed to reach the objective. An episode is an experience where the agent interacts with its environment until either having reached the objective or either having used all attempts. A similar definition of an episode is the consecutive states of the environment.\n\n\nVideo\nAnimation : The agent finds a path to eat the blue dot"
  },
  {
    "objectID": "posts/introduction-reinforcement-learning/index.html#simplifying-of-the-episode",
    "href": "posts/introduction-reinforcement-learning/index.html#simplifying-of-the-episode",
    "title": "Introduction to Reinforcement Learning",
    "section": "Simplifying of the episode",
    "text": "Simplifying of the episode\nIn order to understand how the agent will learn by exploring its environment, the example is simplified to only one step to reach the objective. The agent has only one action per episode allowed. As a result, it will either succeed or either fails.\n\n\n\nFigure : The agent has only one action per episode\n\n\n\nActions\nTo summarize about actions, there are six discrete actions that can counted :\n\n\n\nActions\n\n\n\n\n\\(A_1 = X\\)\n\n\n\\(A_2 = -X\\)\n\n\n\\(A_3 = Y\\)\n\n\n\\(A_4 = -Y\\)\n\n\n\\(A_5 = Z\\)\n\n\n\\(A_6 = -Z\\)\n\n\n\nNote in some environment, the action may be continuous which means the action is included in a range. For instance, if the action is to determine the angular position given different information from sensors, then the expected action is a number (a decimal). Whereas in this example, the output of the model is an array of probabilities of actions.\n\n\nRewards\nThe definition of the rewards is one of the keys of Reinforcement Learning. They denote the amount of success of the agent by taking a specific action. In this way, the agent receives positive and negatives rewards depending on its behavior.\n\n\n\nFigure : Table of actions and rewards\n\n\nIn this example, each action can be seen as the faces of dice if we assume that the agent is not trained and there is as much as chance between all actions. The rewards are defined by the designer in such a way that the agent is led to the objective.\n\n\nVideo\nAnimation : The agent tries different actions and gets rewards\n\n\nNote the reward is discrete (i.e. six scalar values based on actions) whereas it can be defined as the reward function. It is a function often based on the action and the environment."
  },
  {
    "objectID": "posts/introduction-reinforcement-learning/index.html#expected-reward",
    "href": "posts/introduction-reinforcement-learning/index.html#expected-reward",
    "title": "Introduction to Reinforcement Learning",
    "section": "Expected reward",
    "text": "Expected reward\nIn Reinforcement Learning, the model (the agent) has to maximize the expected reward in long term. It is the starting point for learning. The expected reward is defined as: \\[\n\\mathbb E(R(\\tau))\n\\] where \\(\\mathbb E(\\cdot)\\) is the expected value, \\(R(\\cdot)\\) is the reward function and \\(\\tau\\) is the trajectory, a set of consecutive states, actions and rewards.\nA trajectory can be seen as the path taken by the agent (set of consecutive positions and directions) and the rewards obtained through this specific path.\nThe model of the agent \\(\\pi\\) is called the policy. It denotes the current behavior of agent.\nIt is possible to compute the expected value of the policy of the agent based on the probabilities and rewards. \\[\n\\begin{align}\n\\mathbb E(R | \\pi) &= 10 \\times \\frac 1 6 - 1 \\times \\frac 1 6 - 1 \\times \\frac 1 6 - 1 \\times \\frac 1 6 - 1 \\times \\frac 1 6 - 1 \\times \\frac 1 6 \\\\\n                   &= \\frac 5 6 \\approx 0.833\n\\end{align}\n\\]\nBut it can also be seen graphically. The expected reward is the area under the curve of the reward function.\n\n\n\nFigure : Initial Expected Reward\n\n\nWhen the agent chooses different actions, it will modify the probabilities of actions based on its experience. And since its goal is to maximize the expected reward in long term, the area under the curve has to be maximized by changing probabilities.\n\n\nVideo\nAnimation : Evolution of the Expected Reward through training"
  },
  {
    "objectID": "posts/introduction-reinforcement-learning/index.html#final-result",
    "href": "posts/introduction-reinforcement-learning/index.html#final-result",
    "title": "Introduction to Reinforcement Learning",
    "section": "Final result",
    "text": "Final result\nThe agent is able to maximize an expected reward based on its experience. However, the agent will find its own path to reach its objective. It means, in most of the cases, there is an infinite number of paths and the way of going through of specific path, depends on several elements:\n\nthe environment which may cause issue depending on its definition\nthe reward function which impacts considerably the behavior of the agent\nthe policy which needs to be carefully designed\n\n\n\nVideo\nAnimation : The agent has found a path to maximize his reward, but there is an infinite number of paths\n\n\nAn ultimate important aspect of Reinforcement Learning is, although RL is a powerful way to solve problems such as games, long-term planning or robotic problems, in real world applications, the environment is dynamic, which means it changes over time. Therefore, the agent must be trained over time with new data coming from the environment. For instance, for a planning, if the agent needs to deal with a new process, it might fail if the new process is too much different from the known processes."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Different projects",
    "section": "",
    "text": "Computation of the sum of prime numbers in different languages\n\n\n\ncomputing\n\n\n\n\n\n\n\nBenjamin Bourbon\n\n\nJan 20, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/introduction-federated-learning/index.html",
    "href": "posts/introduction-federated-learning/index.html",
    "title": "Introduction to Federated Learning",
    "section": "",
    "text": "Federated Learning (FL) was introduced by McMahan et al. (2017) which has attracted increasing interest of ML researchers. They proposed a decentralized collaborative approach that uses multiple devices to train a global model without sharing their local data. In most cases, devices are limited in resources such as computation and communication and restricted by the usage of the user. Federated Learning can meet expectations of specific fields such as natural language processing, computer vision, health care, transportation, finance, smart city, robotics, networking, blockchain and others. In other words, the fact of training a model without exchanging local data gives an overview of new possibilities for taking advantage of each other while keeping privacy.\n\n\n\nFederated Learning (FL) is a method where multiple devices (called clients) are led to collaborate together with communications to a central server in order to train a global model while protecting privacy. In other words, instead of sharing the local data of users, only updates of local models on clients’ devices are communicated to the server.\nFL can be applied in various situations. For instance, in mobile phones there is a large amount of data, which is private by nature. The data could be acquired through GPS locations, microphones or cameras for example. If we want to build a global model such as takes account all contributions of all mobile phones, there are risks and responsibilities to deal with private data. FL aims to build a joint Machine Learning model (ML) without sharing local data. This technique could help diverse fields to collaborate together in order to train a global model that would be used by all participants. For instance, in hospitals, where privacy about health data, is the most important criterion, they could build an organization to perform an ML model in collaboration and every of them would take advantage of each other while keeping privacy. Another way to see FL application is when the implementation of the algorithm is fully decentralized. In other words, when FL architecture is based on peer-to-peer, communications between the server and devices are not required. For example, this approach offers new types of applications such as attack detection model that could be developed jointly by multiple banks.\n\n\n\n\n\nFor this example, we introduce a server and cellphones as the starting point. The server leads communications with cellphones. Cellphones are the requested devices for this example. The goal is to exploit the private data of cellphones without sharing them to the server.\n\n\n\nFigure : A server and several devices\n\n\nNote on the figure, only five devices are represented, but in a real world application, millions of devices could interact with a server (or an infrastructure) in order to apply the Federated Learning.\n\n\n\nAt the beginning of the FL process, a global model is initialized by the server. It can be a neural network or any kind of models (e.g. a decision tree).\n\n\n\nFigure : The server initializes a global neural network\n\n\n\n\n\nThe FL works with rounds where some devices are selected to train the global model on their local data. Every round, devices are selected randomly or arbitrary based on known information about them. The selection part is a crucial part because some devices could be unavailable for a moment or may have different performances of computation and communication (straggler problem). As well, because the private data is not communicated, the distribution of data is managed with difficulty. Also, it might be impraticable if all devices are selected every round.\n\n\n\nFigure : The server selects a sample of devices\n\n\n\n\n\nOnce devices are selected, the server upload the parameters of the global model for all devices.\n\n\nVideo\nAnimation : The server uploads the global neural network to cellphones\n\n\n\n\n\nEach device trains their local model on their private data.\n\n\nVideo\nAnimation : The cellphones train their neural network on local data\n\n\n\n\n\nThe server collects parameters of all local models.\n\n\nVideo\nAnimation : The server downloads the local trained neural networks from cellphones\n\n\n\n\n\nThe local models are aggregated into a new global model. For instance, it can be done by computing the weighted average of the global loss function :\n\\[\nF_g(w) = \\sum_{i = 1}^N \\eta_i F_i(w)\n\\]\nwhere \\(\\eta_i\\) indicates the relative impact of each part of the global model and \\(F_i(w)\\) is the loss function of each dataset \\(D_i\\). For more information, check McMahan et al. (2017).\n\n\nVideo\nAnimation : The server aggregates the local trained neural networks to compute the next global neural network\n\n\n\n\n\nOnce the new global model is computed, the process is restarted until the objective is reached (for instance by maximizing the accuracy).\n\n\nVideo\nAnimation : The process restarts with a new global neural network\n\n\n\n\n\n\n\nVideo\nAnimation : Complete process of Federated Learning\n\n\n\n\n\n\n\nH. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y. Arcas, “Communication-Efficient Learning of Deep Networks from Decentralized Data,” arXiv:1602.05629 [cs], Feb. 2017, arXiv: 1602.05629. [Online]. Available: http://arxiv.org/abs/1602.05629\nApplication of Reinforcement Learning to improve the selection and the aggregation of Federated Learning (Github)"
  },
  {
    "objectID": "posts/introduction-federated-learning/index.html#introduction",
    "href": "posts/introduction-federated-learning/index.html#introduction",
    "title": "Introduction to Federated Learning",
    "section": "",
    "text": "Federated Learning (FL) was introduced by McMahan et al. (2017) which has attracted increasing interest of ML researchers. They proposed a decentralized collaborative approach that uses multiple devices to train a global model without sharing their local data. In most cases, devices are limited in resources such as computation and communication and restricted by the usage of the user. Federated Learning can meet expectations of specific fields such as natural language processing, computer vision, health care, transportation, finance, smart city, robotics, networking, blockchain and others. In other words, the fact of training a model without exchanging local data gives an overview of new possibilities for taking advantage of each other while keeping privacy."
  },
  {
    "objectID": "posts/introduction-federated-learning/index.html#overview",
    "href": "posts/introduction-federated-learning/index.html#overview",
    "title": "Introduction to Federated Learning",
    "section": "",
    "text": "Federated Learning (FL) is a method where multiple devices (called clients) are led to collaborate together with communications to a central server in order to train a global model while protecting privacy. In other words, instead of sharing the local data of users, only updates of local models on clients’ devices are communicated to the server.\nFL can be applied in various situations. For instance, in mobile phones there is a large amount of data, which is private by nature. The data could be acquired through GPS locations, microphones or cameras for example. If we want to build a global model such as takes account all contributions of all mobile phones, there are risks and responsibilities to deal with private data. FL aims to build a joint Machine Learning model (ML) without sharing local data. This technique could help diverse fields to collaborate together in order to train a global model that would be used by all participants. For instance, in hospitals, where privacy about health data, is the most important criterion, they could build an organization to perform an ML model in collaboration and every of them would take advantage of each other while keeping privacy. Another way to see FL application is when the implementation of the algorithm is fully decentralized. In other words, when FL architecture is based on peer-to-peer, communications between the server and devices are not required. For example, this approach offers new types of applications such as attack detection model that could be developed jointly by multiple banks."
  },
  {
    "objectID": "posts/introduction-federated-learning/index.html#federated-learning-step-by-step",
    "href": "posts/introduction-federated-learning/index.html#federated-learning-step-by-step",
    "title": "Introduction to Federated Learning",
    "section": "",
    "text": "For this example, we introduce a server and cellphones as the starting point. The server leads communications with cellphones. Cellphones are the requested devices for this example. The goal is to exploit the private data of cellphones without sharing them to the server.\n\n\n\nFigure : A server and several devices\n\n\nNote on the figure, only five devices are represented, but in a real world application, millions of devices could interact with a server (or an infrastructure) in order to apply the Federated Learning.\n\n\n\nAt the beginning of the FL process, a global model is initialized by the server. It can be a neural network or any kind of models (e.g. a decision tree).\n\n\n\nFigure : The server initializes a global neural network\n\n\n\n\n\nThe FL works with rounds where some devices are selected to train the global model on their local data. Every round, devices are selected randomly or arbitrary based on known information about them. The selection part is a crucial part because some devices could be unavailable for a moment or may have different performances of computation and communication (straggler problem). As well, because the private data is not communicated, the distribution of data is managed with difficulty. Also, it might be impraticable if all devices are selected every round.\n\n\n\nFigure : The server selects a sample of devices\n\n\n\n\n\nOnce devices are selected, the server upload the parameters of the global model for all devices.\n\n\nVideo\nAnimation : The server uploads the global neural network to cellphones\n\n\n\n\n\nEach device trains their local model on their private data.\n\n\nVideo\nAnimation : The cellphones train their neural network on local data\n\n\n\n\n\nThe server collects parameters of all local models.\n\n\nVideo\nAnimation : The server downloads the local trained neural networks from cellphones\n\n\n\n\n\nThe local models are aggregated into a new global model. For instance, it can be done by computing the weighted average of the global loss function :\n\\[\nF_g(w) = \\sum_{i = 1}^N \\eta_i F_i(w)\n\\]\nwhere \\(\\eta_i\\) indicates the relative impact of each part of the global model and \\(F_i(w)\\) is the loss function of each dataset \\(D_i\\). For more information, check McMahan et al. (2017).\n\n\nVideo\nAnimation : The server aggregates the local trained neural networks to compute the next global neural network\n\n\n\n\n\nOnce the new global model is computed, the process is restarted until the objective is reached (for instance by maximizing the accuracy).\n\n\nVideo\nAnimation : The process restarts with a new global neural network\n\n\n\n\n\n\n\nVideo\nAnimation : Complete process of Federated Learning"
  },
  {
    "objectID": "posts/introduction-federated-learning/index.html#references",
    "href": "posts/introduction-federated-learning/index.html#references",
    "title": "Introduction to Federated Learning",
    "section": "",
    "text": "H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y. Arcas, “Communication-Efficient Learning of Deep Networks from Decentralized Data,” arXiv:1602.05629 [cs], Feb. 2017, arXiv: 1602.05629. [Online]. Available: http://arxiv.org/abs/1602.05629\nApplication of Reinforcement Learning to improve the selection and the aggregation of Federated Learning (Github)"
  },
  {
    "objectID": "posts/introduction-spiking-nn/index.html",
    "href": "posts/introduction-spiking-nn/index.html",
    "title": "Introduction to Spiking Neural Networks",
    "section": "",
    "text": "Deep neural networks (DNN) are certainly one of the major advances of the last decades. On one hand, their performance comes from the computation complexity and the energy consumption. On the other hand, SNN offer models with cheaper computation complexity and a budgetary reduction of the energy consumption. SNN bring excellent performances along for task classification such as on images and sound. This network is often found in time series processing with their ability of classification.\n\n\nSNN have a different approach on information transmission from standard neural networks. They try to imitate biological neural networks. Instead of changing the values over time, SNN work on discrete events which are produced in specific moments. They receive peak series as input and produce time series as output.\n\n\n\nFor every time-step, each neuron has some values which are analogous to a electric potential of biological neurons. This value in the neuron can change based on the mathematical model of the neuron. If the value is higher than a threshold, the neuron sends only one impulse for each neuron downstream of it. Finally, the value of the neuron is set under his mean value. After some time, the value of the neuron is back to the mean value.\n\n\nVideo\nAnimation : Impulse of SNN\n\n\n\n\n\nSNN are built on the mathematical descriptions of biological neurons. There are two groups of methods which are used to model SNN :\n\nmodels based on conductance which describe how actions in neurons are initiated and spread\n\nHodgkin-Huxley model\nFitzHugh–Nagumo model\nMorris–Lecar model\nHindmarsh–Rose model\nIzhikevich model\nCable theory\n\nmodels with a threshold which generate a spike for a given threshold\n\nPerfect Integrate-and-fire\nLeaky Integrate-and-fire\nAdaptive Integrate-and-fire"
  },
  {
    "objectID": "posts/introduction-spiking-nn/index.html#what-are-spiking-neural-networks",
    "href": "posts/introduction-spiking-nn/index.html#what-are-spiking-neural-networks",
    "title": "Introduction to Spiking Neural Networks",
    "section": "",
    "text": "SNN have a different approach on information transmission from standard neural networks. They try to imitate biological neural networks. Instead of changing the values over time, SNN work on discrete events which are produced in specific moments. They receive peak series as input and produce time series as output."
  },
  {
    "objectID": "posts/introduction-spiking-nn/index.html#overview-of-spiking-neural-networks",
    "href": "posts/introduction-spiking-nn/index.html#overview-of-spiking-neural-networks",
    "title": "Introduction to Spiking Neural Networks",
    "section": "",
    "text": "For every time-step, each neuron has some values which are analogous to a electric potential of biological neurons. This value in the neuron can change based on the mathematical model of the neuron. If the value is higher than a threshold, the neuron sends only one impulse for each neuron downstream of it. Finally, the value of the neuron is set under his mean value. After some time, the value of the neuron is back to the mean value.\n\n\nVideo\nAnimation : Impulse of SNN"
  },
  {
    "objectID": "posts/introduction-spiking-nn/index.html#several-models",
    "href": "posts/introduction-spiking-nn/index.html#several-models",
    "title": "Introduction to Spiking Neural Networks",
    "section": "",
    "text": "SNN are built on the mathematical descriptions of biological neurons. There are two groups of methods which are used to model SNN :\n\nmodels based on conductance which describe how actions in neurons are initiated and spread\n\nHodgkin-Huxley model\nFitzHugh–Nagumo model\nMorris–Lecar model\nHindmarsh–Rose model\nIzhikevich model\nCable theory\n\nmodels with a threshold which generate a spike for a given threshold\n\nPerfect Integrate-and-fire\nLeaky Integrate-and-fire\nAdaptive Integrate-and-fire"
  },
  {
    "objectID": "posts/introduction-spiking-nn/index.html#case-where-i-ne-0",
    "href": "posts/introduction-spiking-nn/index.html#case-where-i-ne-0",
    "title": "Introduction to Spiking Neural Networks",
    "section": "Case where \\(I \\ne 0\\)",
    "text": "Case where \\(I \\ne 0\\)\n\n\n\nFigure : RC circuit\n\n\nWe apply the Kirchhoff’s current law on the green point : \\[\nI_e = I_{s_1} + I_{s_2}\n\\] We use the characteristic relation of a resistor (Ohm’s law): \\[\nU = RI\n\\]\nAnd the characteristic relation of a capacitor : \\[\nI = C \\times \\frac {dU}{dt}\n\\]\nWe represent the intensity as \\(I(t)\\) and the tension as \\(V_m(t)\\). Thus we get the following relation: \\[\nI(t) = \\frac{V_m(t)}{R_m} + C_m \\frac{dV(m)}{dt}\n\\] \\[\n\\boxed{C_m \\frac{dV(m)}{dt} = I(t) - \\frac{V_m(t)}{R_m}}\n\\]"
  },
  {
    "objectID": "posts/introduction-spiking-nn/index.html#resolution-of-the-differential-equation",
    "href": "posts/introduction-spiking-nn/index.html#resolution-of-the-differential-equation",
    "title": "Introduction to Spiking Neural Networks",
    "section": "Resolution of the differential equation",
    "text": "Resolution of the differential equation\n\nHomogeneous solution\n\\[\n\\frac{dV(m)}{dt} + \\frac{V_m(t)}{R_m C_m} = 0\n\\]\nWe set \\(\\tau = R_m C_m\\). Then we get: \\[\nV_m(t) = A e^{\\frac{-t}{\\tau}}\n\\] where \\(A\\) is the integration constant of the problem.\n\nParticular solution\nWe assume that \\(I(t)\\) is constant. The particular solution has the same form of the second member of the equation. In other words, \\(\\frac{dV(m)}{dt} = 0\\).\n\\[\n\\implies V_m(t) = R_m I(t)\n\\]\n\n\nGeneral solution\n\\[\nV_m(t) = A e^{\\frac{-t}{\\tau}} + R_m I(t)\n\\]\nWe assume that \\(V_m(0^+) = V_m(0^-) = 0\\). Thus, we get: \\[\nA + R_m I = 0 \\iff A = - R_m I\n\\]\nSo:\n\\[\n\\boxed{V_m(t) = R_m I(1 - e^{\\frac{-t}{\\tau}})}\n\\]"
  },
  {
    "objectID": "posts/introduction-spiking-nn/index.html#calculation-of-the-firing-frequency",
    "href": "posts/introduction-spiking-nn/index.html#calculation-of-the-firing-frequency",
    "title": "Introduction to Spiking Neural Networks",
    "section": "Calculation of the firing frequency",
    "text": "Calculation of the firing frequency\nThe model can become more precise by introducing a refractory time in which the neuron can not be discharged. We are interesting to evaluate the frequency when \\(I &gt; I_{th}\\) (when \\(V_m(t)\\) is constant, \\(I_{th} = \\frac{V_{th}}{R_m}\\)): \\[\n\\begin{align}\n         & V_{th} = R_m I(1 - e^{\\frac{-(t - t_{ref})}{\\tau}}) \\\\\n    \\iff & e^{\\frac{-(t - t_{ref})}{\\tau}} = 1 - \\frac{V_{th}}{I R_m} \\\\\n    \\iff & \\frac{-(t - t_{ref})}{\\tau} = \\log{(1 - \\frac{V_{th}}{I R_m})} \\\\\n    \\iff & t = t_{ref} - \\tau \\log{(1 - \\frac{V_{th}}{I R_m})}\n\\end{align}\n\\]\nThen, we can define the firing frequency with the inverse of the total gap between the impulses (including the down-time). The firing frequency is then :\n\\[\n\\boxed{\n    f(I) = \\left\\{\\begin{array}{ll}\n    0, & I \\le I_{th} \\\\\n    \\left[t_{ref} - \\tau \\log{\\left(1 - \\frac{V_{th}}{I R_m}\\right)}\\right]^{-1}, & I &gt; I_{th}\n    \\end{array}\\right.\n}\n\\]\n\nCase where \\(I = 0\\)\n\n\n\nFigure : RC Circuit\n\n\nWe apply the Kirchhoff’s current law on the green point : \\[\nI_{s_1} + I_{s_2} = 0\n\\]\nWe get the following relation : \\[\n\\frac{V_m(t)}{R_m} + C_m \\frac{dV(m)}{dt} = 0\n\\] \\[\n\\boxed{\\frac{dV(m)}{dt} + \\frac{V_m(t)}{\\tau} = 0}\n\\]\nwith \\(\\tau = R_m C_m\\)\n\n\nSolution of the differential equation\n\\[\nV_m(t) = A e^{\\frac{-t}{\\tau}}\n\\]\nwhere \\(A\\) is the integration constant of the problem.\nWe assume \\(V_m(0) = I R_m\\). Thus : \\[\nA = I R_m\n\\]\nWe get then :\n\\[\n\\boxed{V_m(t) = I R_m e^{\\frac{-t}{\\tau}}}\n\\]"
  },
  {
    "objectID": "posts/introduction-spiking-nn/index.html#conclusion",
    "href": "posts/introduction-spiking-nn/index.html#conclusion",
    "title": "Introduction to Spiking Neural Networks",
    "section": "Conclusion",
    "text": "Conclusion\nWe recall that \\(\\tau\\) is characterized of the duration which makes the start-up level to disappear to be replaced by the permanent permanent. The permanent level is reached after several \\(\\tau\\) (\\(\\approx 5 \\tau\\)). We can do the analogy \\(V_m\\), the tension of bounds of the cellular membrane and \\(R_m\\) the membrane resistance.\n\n\n\nImpulse\n\n\nBenefits - the model will not keep an increase of the tension for ever contrary to other models without leak where it is kept until the appearance of a new impulse.\nDrawbacks - the model does not take into account the neuronal adaptation, so that it can not describe spike series."
  },
  {
    "objectID": "posts/introduction-spiking-nn/index.html#architecture-of-a-snn",
    "href": "posts/introduction-spiking-nn/index.html#architecture-of-a-snn",
    "title": "Introduction to Spiking Neural Networks",
    "section": "Architecture of a SNN",
    "text": "Architecture of a SNN\nEven if SNN have an unique concept, they stay a neural network. We can find :\n\nFeedforward Neural Network\nRecurrent Neural Network\nSynfire chain\nReservoir computing"
  },
  {
    "objectID": "posts/introduction-spiking-nn/index.html#spike-timing-dependent-plasticity-stdp",
    "href": "posts/introduction-spiking-nn/index.html#spike-timing-dependent-plasticity-stdp",
    "title": "Introduction to Spiking Neural Networks",
    "section": "Spike-Timing-Dependent Plasticity (STDP)",
    "text": "Spike-Timing-Dependent Plasticity (STDP)\nIt is a unsupervised learning mechanism. The training is realized layer by layer, in other words, the training of the current layer is made when the training of the previous layer is finished. Neurons of the layer compete with each other and those which fire quickly, trigger a STDP and learn from inputs:\n\\[\n\\Delta \\omega_{ij} =\n\\left\\{\n\\begin{array}{c}\na^+ \\omega_{ij}(1 - \\omega_{ij}), & si & t_j - t_i \\le 0 \\\\\na^- \\omega_{ij}(1 - \\omega_{ij}), & si & t_j - t_i &gt; 0\n\\end{array}\n\\right.\n\\] where \\(i\\) and \\(j\\) respectively refer at the index of postsynaptic neurons and presynaptic neurons, \\(t_i\\) and t_j$ are the time related of spikes, \\(\\Delta \\omega_{ij}\\) is the modification of synaptic weights and \\(a^+\\) and \\(a^-\\) are two specific learning rates.\nWe measure the learning convergence of the \\(l\\)-nth layer under the shape :\n\\[\nC_l = \\sum_f \\sum_i \\frac{\\omega_{f,i}(1 - \\omega_{f,i})}{n_\\omega}\n\\]\nwhere \\(C_l\\) tends to \\(0\\) if each of synaptic weights converge to \\(0\\) or \\(1\\). Therefore, we stop the \\(l\\)-nth layer learning when \\(C_l\\) is enough close to \\(0\\) (i.e. \\(C_l &lt; 0.01\\))."
  },
  {
    "objectID": "projects/performance/index.html",
    "href": "projects/performance/index.html",
    "title": "Computation of the sum of prime numbers in different languages",
    "section": "",
    "text": "Description\nThe project allows to see the difference of speed of program’s execution between multiple languages with one algorithm which computes the sum of prime numbers. Also, the goal is to use native libraries for each language.\nHere is the source of the algorithm\n\n\nCode\n\n       \nimport math\nfrom time import perf_counter\n\n\ndef chrono(function):\n    def inner(*args):\n        start = perf_counter()\n        result = function(*args)\n        duration = perf_counter() - start\n        print(\"%0.3f x 10e(-3) seconds\" % (duration * 1_000))\n        return result\n\n    return inner\n\n\n# @chrono\ndef P10(n):\n    r = int(n ** 0.5)\n    assert r * r &lt;= n and (r + 1) ** 2 &gt; n\n    V = [n // i for i in range(1, r + 1)]\n    V += list(range(V[-1] - 1, 0, -1))\n    S = {i: i * (i + 1) // 2 - 1 for i in V}\n    for p in range(2, r + 1):\n        if S[p] &gt; S[p - 1]:  # p is prime\n            sp = S[p - 1]  # sum of primes smaller than p\n            p2 = p * p\n            for v in V:\n                if v &lt; p2:\n                    break\n                S[v] -= p * (S[v // p] - sp)\n    return S[n]\n\n\ndef _format(dist, value):\n    return value + \" \" * (dist - len(value))\n\n\nprint(\"Power | Time (µs) | Resultat\")\nprint(\"========================================\")\nfor i in range(1, 9):\n    start = perf_counter()\n    result = P10(10 ** i)\n    duration = int((perf_counter() - start) * 1_000_000)\n    print(_format(5, str(i)) + \" | \" + _format(9, str(duration)) + \" | \" + str(result))\nuse std::collections::HashMap;\nuse std::time::Instant;\n\ntype Dict = HashMap&lt;u64, u128&gt;;\n\nfn get_keys(n: u64, half_size: u64) -&gt; Vec&lt;u64&gt; {\n    // Calculation of keys used in the HashMap\n    let mut v: Vec&lt;u64&gt; = Vec::new();\n    for i in 0..half_size + 1 {\n        v.push(n / (i + 1));\n    }\n    let v_max = v[half_size as usize];\n    for i in (1..v_max).rev() {\n        v.push(i);\n    }\n    v\n}\n\nfn get_sums(keys: &[u64]) -&gt; Dict {\n    // Initialisation du hashmap\n    let mut hmap = HashMap::new();\n    for key in keys.iter() {\n        let big_key = *key as u128;\n        let value = (big_key * (big_key + 1)) / 2 - 1;\n        hmap.insert(*key, value);\n    }\n    hmap\n}\n\nfn calculate_sums(mut hmap: Dict, keys: &[u64], square_n: u64) -&gt; Dict {\n    for p in 2..square_n + 1 {\n        let current_sum = hmap[&(p - 1)];\n        if hmap[&p] &gt; current_sum {\n            let p_square = p * p;\n            for key in keys.iter() {\n                if *key &lt; p_square {\n                    break;\n                }\n                let new_value = hmap[&key] - (p as u128) * (hmap[&(key / p)] - current_sum);\n                hmap.insert(*key, new_value);\n            }\n        }\n    }\n    hmap\n}\n\nfn primes(n: u64) -&gt; u128 {\n    let square_n = (n as f64).sqrt() as u64;\n    assert!(square_n * square_n &lt;= n && (square_n + 1).pow(2) &gt; n);\n    let half_size = square_n - 1;\n    let keys = get_keys(n, half_size);\n    let p_keys = &keys;\n    let mut all_sums = get_sums(p_keys);\n    all_sums = calculate_sums(all_sums, p_keys, square_n);\n    all_sums[&n]\n}\n\nfn format(dist: usize, value: &String) -&gt; String {\n    let mut v: String = value.to_string();\n    for _ in 1..(dist - (*value).len()) {\n        v += &\" \";\n    }\n    return v;\n}\n\nfn main() {\n    println!(\"Power | Time (µs) | Resultat\");\n    println!(\"========================================\");\n    for i in 1..9 {\n        let now = Instant::now();\n        let n: u64 = (10_u64).pow(i);\n        let value: u128 = primes(n);\n        let new_now = Instant::now();\n        println!(\n            \"{} | {} | {}\",\n            format(6, &(i.to_string())),\n            format(10, &(new_now.duration_since(now).as_micros().to_string())),\n            value\n        );\n    }\n}\n#include &lt;iostream&gt;\n#include &lt;cstdio&gt;\n#include &lt;cmath&gt;\n#include &lt;assert.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unordered_map&gt;\n#include &lt;chrono&gt;\n#include &lt;string&gt;\nusing namespace std::chrono;\nusing namespace std;\n\ntypedef unordered_map&lt;long, long long&gt; umap;\n\nvoid generate_key_array(long long key_array[], long long n, long half_size){\n  // generation of a key array which serves for the dictionary all_sums\n  long long value;\n\n  for(long index = 0; index &lt;= half_size+1; index++){\n    value = (long long) n / (index + 1);\n    key_array[index] = value;\n  }\n\n  for(long index = half_size + 1; index &lt; 2 * half_size + 1; index++){\n    value = key_array[index - 1] - 1;\n    key_array[index] = value;\n  }\n}\n\nvoid generate_all_sums(umap& all_sums, long long key_array[], long size){\n  // generation of dictionary all_sums\n  long long value;\n\n  for(long index = 0; index &lt; size; index++){\n    value = key_array[index];\n    // storing sum of numbers for 1 to key\n    all_sums[value] = ((long long) (value*(value + 1))/2) - 1;\n  }\n}\n\n\nvoid calculate_new_sums(umap& all_sums, long long key_array[], long long square_root){\n  // core of the program for calculate the sum of prime numbers\n  long long current_sum;\n  long long p_square;\n  long index;\n  long long key;\n  long long second_key;\n\n  for(long p = 2; p &lt;= square_root; p++){\n    current_sum = all_sums[p - 1];\n    if(all_sums[p] &gt; current_sum){ //if p is prime\n      p_square = p * p;\n      index = 0;\n      key = key_array[index];\n\n      // while the key is lower than p * p\n      while(key &gt;= p_square){\n        second_key = key / p;\n        // updating the sum of the key 'key'\n        all_sums[key] -= p * (all_sums[second_key] - current_sum);\n        index++;\n        key = key_array[index];\n      }\n    }\n  }\n}\n\nlong long P10(long long n){\n  long long square_root = (long long) sqrt(n);\n  //check if the program will work\n  assert ((square_root*square_root &lt;= n) && ((square_root + 1) * (square_root + 1) &gt; n));\n\n  long long final_sum;\n  long half_size = square_root - 1;\n\n  long long* key_array = new long long [half_size * 2 + 1];\n  generate_key_array(key_array, n, half_size);\n\n  long size = 2 * half_size + 1;\n  // dictionary of sums\n  umap all_sums;\n  generate_all_sums(all_sums, key_array, size);\n  calculate_new_sums(all_sums, key_array, square_root);\n\n  final_sum = all_sums[n];\n  delete key_array;\n  all_sums.clear();\n\n  return final_sum;\n}\n\nstring format(int dist, string value){\n  string hole = \"\";\n  for(int _=0; _&lt;=(dist-(int)(value).size()); _++){\n    hole+=\" \";\n  }\n  return value + hole;\n}\n\n\nint main(){\n  // Goal: 100 000 000 000\n  // Awaited result: 201 467 077 743 744 681 014\n  auto start = high_resolution_clock::now();\n  long long value;\n  cout &lt;&lt; \"Power | Time (µs) | Result\" &lt;&lt; endl;\n  cout &lt;&lt; \"========================================\" &lt;&lt; endl;\n  for(int i=1; i&lt;=8; i++){\n    // cout &lt;&lt; \"Power: \" &lt;&lt; i &lt;&lt; endl;\n    value = P10(pow(10,i));\n    auto stop = high_resolution_clock::now();\n    auto duration = duration_cast&lt;microseconds&gt;(stop - start);\n    // cout &lt;&lt; value &lt;&lt; endl;\n    // cout &lt;&lt; \"Time taken by function: \" &lt;&lt; duration.count() &lt;&lt; \" x 10e(-3) seconds\" &lt;&lt; endl;\n    cout &lt;&lt; format(4, std::to_string(i)) &lt;&lt; \" | \" &lt;&lt; format(8, to_string(duration.count())) &lt;&lt; \" | \" &lt;&lt; value &lt;&lt; endl;\n  }\n  return 0;\n}\ndefmodule Primes do\n  @spec sqrt(integer) :: integer\n  defp sqrt(n) do\n    trunc(:math.sqrt(n))\n  end\n\n  @spec head_keys(integer, integer) :: [integer]\n  defp head_keys(n, root_n) do\n    1..(root_n + 1)\n    |&gt; Enum.map(&div(n, &1))\n  end\n\n  @spec tail_keys(integer, integer) :: [integer]\n  defp tail_keys(n, root_n) do\n    Enum.to_list(div(n, root_n)..1)\n  end\n\n  @spec generate_keys(integer, integer) :: [integer]\n  defp generate_keys(n, root_n) do\n    head_keys(n, root_n) ++ tail_keys(n, root_n)\n  end\n\n  @spec n_sums(integer) :: {integer}\n  defp n_sums(i) do\n    {i, div(i * (i + 1), 2) - 1}\n  end\n\n  @spec generate_sums([integer]) :: %{integer =&gt; integer}\n  defp generate_sums(keys) do\n    Enum.into(keys, %{}, &n_sums(&1))\n  end\n\n  @spec small(integer, %{integer =&gt; integer}, integer, integer) :: {integer}\n  defp small(v, dict, p, sp) do\n    {v, dict[v] - p * (dict[div(v, p)] - sp)}\n  end\n\n  @spec calculate(%{integer =&gt; integer}, [integer], integer, integer) :: %{integer =&gt; integer}\n  defp calculate(sums, keys, p, limit) when p &lt; limit do\n    sum_p = sums[p - 1]\n\n    if sums[p] &gt; sum_p do\n      Enum.take_while(keys, &(&1 &gt;= p * p))\n      |&gt; Enum.into(%{}, &small(&1, sums, p, sum_p))\n      |&gt; (&Map.merge(sums, &1)).()\n      |&gt; calculate(keys, p + 1, limit)\n    else\n      calculate(sums, keys, p + 1, limit)\n    end\n  end\n\n  @spec calculate(%{integer =&gt; integer}, [integer], integer, integer) :: %{integer =&gt; integer}\n  defp calculate(sums, _, p, limit) when p &gt;= limit do\n    sums\n  end\n\n  @spec sum_up_to(integer) :: integer\n  def sum_up_to(n) do\n    root_n = sqrt(n)\n    keys = generate_keys(n, root_n)\n    sums = generate_sums(keys)\n    calculate(sums, keys, 2, root_n + 1)[n]\n  end\n\n  @spec measure(function) :: integer\n  def measure(function) do\n    function\n    |&gt; :timer.tc()\n    |&gt; elem(0)\n    # divide by 1_000 for milliseconds\n    |&gt; Kernel./(1)\n  end\nend\n\ndefmodule Formatter do\n  @spec add_space(integer, charlist) :: charlist\n  defp add_space(0, string) do\n    string\n  end\n\n  @spec add_space(integer, charlist) :: charlist\n  defp add_space(i, string) do\n    add_space(i - 1, string &lt;&gt; \" \")\n  end\n\n  @spec format(integer, integer) :: charlist\n  defp format(dist, value) do\n    string_v = \"#{value}\"\n    add_space(dist - String.length(string_v), string_v)\n  end\n\n  @spec format_all(integer, integer, integer) :: nil\n  def format_all(power, time, value) do\n    IO.puts(\"#{format(5, power)} | #{format(9, time)} | #{value}\")\n  end\nend\n\nIO.puts(\"Power | Time (µs) | Result\")\nIO.puts(\"=========================================\")\n\nPrimes.sum_up_to(100)\n\n1..8\n|&gt; Enum.map(fn exp -&gt;\n  f = Primes.measure(fn -&gt; Primes.sum_up_to(:math.pow(10, exp) |&gt; round) end)\n  Formatter.format_all(exp, f |&gt; round, Primes.sum_up_to(:math.pow(10, exp) |&gt; round))\nend)\nimport java.util.HashMap;\nimport java.util.Vector;\nimport java.lang.Math;\n\npublic class Main{\n\n    static Vector&lt;Integer&gt; get_keys(Integer n, Integer half_size){\n        Vector&lt;Integer&gt; V = new Vector&lt;&gt;();\n        for (Integer i = 0; i &lt; half_size; i++){\n            V.add(n / (i + 1));\n        }\n        Integer vmax = V.get(V.size() - 1) - 1;\n        for (Integer i = vmax; i &gt; 0; i--){\n            V.add(i);\n        }\n        return V;\n    }\n\n    static HashMap&lt;Integer, Long&gt; get_sums(Vector&lt;Integer&gt; keys){\n        HashMap&lt;Integer, Long&gt; hmap = new HashMap&lt;&gt;();\n        for (int key: keys){\n            long big_key = key;\n            long value = (big_key * (big_key + 1)) / 2 - 1;\n            hmap.put(key, value);\n        }\n        return hmap;\n    }\n\n    static HashMap&lt;Integer, Long&gt; calculate_sums(HashMap&lt;Integer, Long&gt; hmap, Vector&lt;Integer&gt; keys, int square_n){\n        for (Integer p = 2; p &lt; square_n + 1; p++){\n            long current_sum = hmap.get(p - 1);\n            if (hmap.get(p) &gt; current_sum){\n                long p_square = p * p;\n                for (int key: keys){\n                    if (key &lt; p_square){\n                        break;\n                    }\n                    long new_value = hmap.get(key) - (long)p * (hmap.get(key / p) - current_sum);\n                    hmap.put(key, new_value);\n                }\n            }\n        }\n        return hmap;\n    }\n\n    static long primes(int n){\n        int square_n = (int) Math.sqrt(n);\n        assert square_n * square_n &lt;= n && (square_n + 1) * (square_n + 1) &gt; n;\n        int half_size = square_n - 1;\n        Vector&lt;Integer&gt; keys = Main.get_keys(n, half_size);\n        HashMap&lt;Integer, Long&gt; all_sums = Main.get_sums(keys);\n        all_sums = Main.calculate_sums(all_sums, keys, square_n);\n        return all_sums.get(n);\n    }\n\n    static String format(int dist, String value){\n        return value + \" \".repeat(dist - value.length());\n    }\n\n    public static void main(String[] args){\n        Main App = new Main();\n        System.out.println(\"Power | Time (µs) | Resultat\");\n        System.out.println(\"========================================\");\n        primes(100);\n        for (int i = 1; i &lt; 9; i++){\n            long start = System.nanoTime();\n            long result = primes((int) Math.pow(10, i));\n            long duration = (System.nanoTime() - start) / 1_000;\n            System.out.println(\n                Main.format(5, String.valueOf(i)) + \" | \" +\n                Main.format(9, String.valueOf(duration)) + \" | \" +\n                String.valueOf(result)\n            );\n        }\n    }\n}\nfunction P10(n)\n    r = math.floor(n^0.5)\n    assert(r * r &lt;= n and (r + 1)^2 &gt; n)\n    V = {}\n    for i = 1, r + 1, 1 do\n        table.insert(V, n // i)\n    end\n    for i = V[#V] - 1, 1, -1 do\n        table.insert(V, i)\n    end\n    S = {}\n    for _, i in ipairs(V) do\n        S[i] = i * (i + 1) // 2 - 1\n    end\n    for p = 2, r, 1 do\n        if S[p] &gt; S[p - 1] then\n            sp = S[p - 1]\n            p2 = p * p\n            for _, v in ipairs(V) do\n                if v &lt; p2 then\n                    break\n                end\n                S[v] = S[v] - p * (S[v // p] - sp)\n            end\n        end\n    end\n    return S[n]\nend\n\nfunction format(dist, value)\n    return value .. string.rep(\" \", (dist - string.len(value)))\nend\n\nprint(\"Power | Time (µs) | Resultat\")\nprint(\"========================================\")\nfor i = 1, 8, 1 do\n    start = os.clock()\n    result = P10(math.floor(10^i))\n    duration = math.floor((os.clock() - start) * 10^6)\n    print(format(5, tostring(i)) .. \" | \" .. format(9, tostring(duration)) .. \" | \" .. tostring(result))\nend\nconst {performance} = require(\"perf_hooks\");\n\nfunction P10(n){\n    var r = Math.floor(n ** 0.5);\n    console.assert(r * r &lt;= n && Math.pow(r + 1, 2) &gt; n);\n    var V = [];\n    for (let i = 1; i &lt; r + 1; i++){\n        V.push(Math.floor(n / i));\n    }\n    for (let i = V[V.length - 1] - 1; i &gt; 0; i--){\n        V.push(i);\n    }\n    var S = V.reduce((S, i) =&gt; {\n        S[i] = Math.floor(i * (i + 1) / 2) - 1\n        return S;\n    }, {});\n    var length = V.length;\n    for (let p = 2; p &lt; r + 1; p++){\n        if (S[p] &gt; S[p - 1]) {\n            var sp = S[p - 1];\n            var p2 = p * p;\n            for (let index = 0; index &lt; length; index++){\n                let v = V[index];\n                if (v &lt; p2){\n                    break;\n                }\n                S[v] -= p * (S[Math.floor(v / p)] - sp);\n            }\n        }\n    }\n    return S[n];\n}\n\nfunction format(dist, value){\n    return value + \" \".repeat(dist - value.length)\n}\n\nconsole.log(\"Power | Time (µs) | Resultat\")\nconsole.log(\"========================================\")\nP10(100)\nfor (let i=1; i&lt;9; i++){\n    var start = performance.now();\n    var result = P10(Math.floor(Math.pow(10, i)));\n    var duration = Math.floor((performance.now() - start) * 1_000);\n    console.log(format(5, String(i)) + \" | \" + format(9, String(duration)) + \" | \" + String(result));\n}\nimport scala.collection.mutable.HashMap\nimport scala.collection.immutable.Vector\nimport scala.math.sqrt\nimport scala.math.pow\nimport scala.math.BigInt\n\ndef get_keys(n: Long, half_size:Long): Vector[Long] =\n  var v = Vector.range(0L, half_size + 1L).map(i =&gt; n / (i + 1))\n  val vmax = v(half_size.asInstanceOf[Int])\n  v ++ Vector.range(1L, vmax).reverse\n\ndef get_sums(keys: Vector[Long]) : HashMap[Long, BigInt] =\n  var hmap = HashMap.empty[Long, BigInt]\n  for key &lt;- keys do\n    val k = BigInt(key)\n    val value = (k * (k + 1)) / 2 - 1\n    hmap.put(key, value)\n  hmap \n\ndef calculate_sums(hmap: HashMap[Long, BigInt], keys: Vector[Long], square_n: Long): HashMap[Long, BigInt] =\n  var cmap = hmap.clone()\n  for p &lt;- (2L to square_n) do\n    val current_sum = cmap(p - 1)\n    if (cmap(p) &gt; current_sum) {\n      val p_square = p * p\n      for key &lt;- keys.takeWhile(p =&gt; p &gt;= p_square) do\n        val new_value = cmap(key) - p * (cmap(key / p) - current_sum)\n        cmap.update(key, new_value)\n    }\n  cmap\n\ndef primes(n: Long): BigInt =\n  val square_n = sqrt(n.asInstanceOf[Float]).asInstanceOf[Long]\n  val half_size = square_n - 1\n  val keys = get_keys(n, half_size)\n  var all_sums = get_sums(keys)\n  all_sums = calculate_sums(all_sums, keys, square_n)\n  all_sums(n)\n\ndef add_space(i:Int, string:String) : String =\n  if (i == 0){\n    return string\n  } else {\n    return add_space(i - 1, string.concat(\" \"))\n  }\n\ndef format(dist:Int, string: String): String =\n  add_space(dist - string.length(), string)\n\n@main def main() =\n  val result = primes(10)\n  println(\"Power | Time (µs) | Resultat\")\n  println(\"========================================\")\n  for power &lt;- (1 to 8) do\n    val start = System.nanoTime\n    val n = pow(10.0, power.asInstanceOf[Double]).asInstanceOf[Long]\n    val result = primes(n)\n    val duration = ((System.nanoTime - start) / 1e3d).asInstanceOf[Int]\n    println(s\"${format(5, s\"$power\")} | ${format(9, s\"$duration\")} | $result\")\n\n\nGraph\nThe first graph is the execution time (in \\(\\mu s\\)) given the power of \\(10^x\\). The second graph is the execution time (in \\(\\log(\\mu s)\\)) after the application of the logarithm function given the power of \\(10^x\\). You can interact with the legend (for instance, click on elixir).\n\n\n\n\n\n\n\n\nNotes\nThe idea comes from the challenge Euler problem n°245 where one part of the problem involves to compute the sum of prime numbers up to \\(10^{11}\\).\nGitHub Repository"
  }
]