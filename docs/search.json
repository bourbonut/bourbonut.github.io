[
  {
    "objectID": "posts/involute-spur-gear/index.html",
    "href": "posts/involute-spur-gear/index.html",
    "title": "Spur Tooth Profile",
    "section": "",
    "text": "Involute gears are the most popular power transmission devices. We find them as an essential component for many machines. For instance, epicyclic gear train is a combination of gears which allows to make a reduction in a compact way. Spur gears are the most popular form and the most efficient type of gearing."
  },
  {
    "objectID": "posts/involute-spur-gear/index.html#code",
    "href": "posts/involute-spur-gear/index.html#code",
    "title": "Spur Tooth Profile",
    "section": "Code",
    "text": "Code\nfrom collections import namedtuple\nfrom functools import partial\nfrom manim import * # pip install manim\n\ndef profile(m, z, alpha=radians(20), ka=1, kf=1.25, interference=True):\n    # Parameters\n    ha = m * ka  #                  addendum height\n    hf = m * kf  #                  dedendum height\n    p = pi * m  #                   step\n    rp = pitch_radius(m, z) #       pitch radius\n    ra = rp + ha #                  addendum radius\n    rf = rp - hf #                  dedendum radius\n    rb = base_radius(m, z, alpha) # base radius\n\n    ta = angle_involute(ra, rb) #   addendum angle\n    tp = angle_involute(rp, rb) #   pitch angle\n\n    duplicate = (\n        lambda obj, angle: obj.copy()\n        .apply_matrix(mat3(X, -Y, Z))\n        .rotate_about_origin(angle)\n    )\n    phase = pi / z + 2 * (tp - atan2(tp, 1))\n\n    # Involute\n    side = ParametricFunction(partial(involute, r=rb), t_range=[0, ta])\n\n    # Arc parameters\n    ArcParameters = namedtuple(\"ArcParameters\", [\"center\", \"radius\", \"angle\"])\n    r = 0.5 * (rb - rf)\n    t = -atan2(r, rf + r)\n    arcp = ArcParameters((rf + r) * u(t), r, t)\n    arc = Arc(arcp.radius, -pi + arcp.angle, -pi / 2, arc_center=arcp.center)\n\n    # Joint, top and bottom\n    angle_top = ta - atan2(ta, 1)\n    top = Arc(ra, angle_top, phase - 2 * angle_top)\n    joint = Line(arcp.center + arcp.radius * u(-3 * pi / 2 + arcp.angle), rb * X)\n    M = arcp.center + arcp.radius * u(-pi + arcp.angle)\n    angle_bottom = anglebt(M, u(0.5 * phase)) * 2\n    bottom = Line(M, rotation(-2 * pi / z + angle_bottom) * M)\n\n    # Patches\n    top_dot = Dot(ra * u(angle_top), radius=0.02)\n    side_dot = Dot(rb * X, radius=0.02)\n    point = arcp.center + arcp.radius * u(-3 * pi / 2 + arcp.angle)\n    joint_dot = Dot(point, radius=0.02)\n    bottom_dot = Dot(M, radius=0.02)\n    dots = (top_dot, side_dot, joint_dot, bottom_dot)\n\n    # Duplicated objects\n    duplicated_objs = map(\n        partial(duplicate, angle=phase), (side, arc, joint) + dots\n    )\n\n    return VGroup(\n        side, top, arc, joint, bottom, *duplicated_objs, *dots\n    ).rotate_about_origin(-phase * 0.5)\nNote : for manim, see installation"
  },
  {
    "objectID": "posts/involute-spur-gear/index.html#code-1",
    "href": "posts/involute-spur-gear/index.html#code-1",
    "title": "Spur Tooth Profile",
    "section": "Code",
    "text": "Code\nfrom collections import namedtuple\nfrom functools import partial\nfrom manim import *\n\ndef involute(t, r, t0=0):\n    return r * (u(t - t0) - t * v(t - t0))\n\n\ndef interference_curve(t, r, x, y, t0):\n    return involute(t, r, t0) - x * u(t - t0) + y * v(t - t0)\n\n\ndef derived_involute(t, r, t0):\n    return r * t * u(t - t0)\n\n\ndef derived_interference_curve(t, r, x, y, t0):\n    return derived_involute(t, r, t0) - x * v(t - t0) - y * u(t - t0)\n\n\ndef jacobian_involute(rb, rp, x, y, t0):\n    return lambda t1, t2: mat3(\n        derived_involute(t1, rb, t0), \n        -derived_interference_curve(t2, rp, x, y, t0),\n        Z,\n    )\n\ndef angle_involute(r, rb):\n    return sqrt(r * r / (rb * rb) - 1)\n\ndef profile(m, z, alpha=radians(20), ka=1, kf=1.25, interference=True):\n    # Parameters\n    ha = m * ka  #                  addendum height\n    hf = m * kf  #                  dedendum height\n    p = pi * m  #                   step\n    rp = pitch_radius(m, z) #       pitch radius\n    ra = rp + ha #                  addendum radius\n    rf = rp - hf #                  dedendum radius\n    rb = base_radius(m, z, alpha) # base radius\n\n    ta = angle_involute(ra, rb) #   addendum angle\n    tp = angle_involute(rp, rb) #   pitch angle\n\n    duplicate = (\n        lambda obj, angle: obj.copy()\n        .apply_matrix(mat3(X, -Y, Z))\n        .rotate_about_origin(angle)\n    )\n\n    la = rack.addendum_length(m, alpha, ka)\n    ts = tp - atan2(tp, 1)\n    phase = pi / z + 2 * (tp - atan2(tp, 1))\n    phase_empty = 2 * pi / z - phase\n    angle_top = ta - atan2(ta, 1)\n    tmin = la * 0.5 / rp\n\n    Functions = namedtuple(\"Functions\", [\"involute\", \"interference\"])\n    functions = Functions(\n        partial(involute, r=rb),\n        partial(interference_curve, r=rp, x=ha, y=0.5 * la, t0=phase_empty * 0.5),\n    )\n\n    # Newton method\n    f = lambda t1, t2: functions.involute(t1) - functions.interference(t2)\n    J = jacobian_involute(rb, rp, ha, 0.5 * la, phase_empty * 0.5)\n    # t3 is not used, but exists because 3D vectors\n    t1, t2, t3 = 0.5 * ta, -0.5 * ta, 0\n    for i in range(8):\n        t1, t2, t3 = vec3(t1, t2, t3) - inverse(J(t1, t2)) * f(t1, t2)\n\n    # Involute and interference curve\n    side = ParametricFunction(functions.involute, t_range=[t1, ta])\n    interference = ParametricFunction(functions.interference, t_range=[t2, tmin])\n\n    # Top and bottom\n    top = Arc(ra, angle_top, phase - 2 * angle_top)\n    M = functions.interference(tmin)\n    angle_bottom = anglebt(M, u(0.5 * phase)) * 2\n    bottom = Line(M, rotation(-2 * pi / z + angle_bottom) * M)\n\n    # Patches\n    top_dot = Dot(ra * u(angle_top), radius=0.02)\n    interference_dot = Dot(functions.interference(t2), radius=0.02)\n    involute_dot = Dot(functions.involute(t1), radius=0.02)\n    bottom_dot = Dot(M, radius=0.02)\n    dots = (top_dot, interference_dot, involute_dot, bottom_dot)\n\n    # Duplicated objects\n    duplicated_objs = map(\n        partial(duplicate, angle=phase), (side, interference) + dots\n    )\n\n    return VGroup(\n        side, interference, *duplicated_objs, top, bottom, *dots\n    ).rotate_about_origin(-phase * 0.5)\nAnd the gear profile :\n\n\n\nFigure : Gear profile with interference\n\n\nNote the closest radius to the pinion’s center is greater than the dedendum radius due to the interference curve.\nLet’s see the rack movement now :\n\n\nVideo\nAnimation : Rack moving around the pinion"
  },
  {
    "objectID": "posts/introduction-spiking-nn/index.html",
    "href": "posts/introduction-spiking-nn/index.html",
    "title": "Introduction to Spiking Neural Networks",
    "section": "",
    "text": "Deep neural networks (DNN) are certainly one of the major advances of the last decades. On one hand, their performance comes from the computation complexity and the energy consumption. On the other hand, SNN offer models with cheaper computation complexity and a budgetary reduction of the energy consumption. SNN bring excellent performances along for task classification such as on images and sound. This network is often found in time series processing with their ability of classification.\n\n\nSNN have a different approach on information transmission from standard neural networks. They try to imitate biological neural networks. Instead of changing the values over time, SNN work on discrete events which are produced in specific moments. They receive peak series as input and produce time series as output.\n\n\n\nFor every time-step, each neuron has some values which are analogous to a electric potential of biological neurons. This value in the neuron can change based on the mathematical model of the neuron. If the value is higher than a threshold, the neuron sends only one impulse for each neuron downstream of it. Finally, the value of the neuron is set under his mean value. After some time, the value of the neuron is back to the mean value.\n\n\nVideo\nAnimation : Impulse of SNN\n\n\n\n\n\nSNN are built on the mathematical descriptions of biological neurons. There are two groups of methods which are used to model SNN :\n\nmodels based on conductance which describe how actions in neurons are initiated and spread\n\nHodgkin-Huxley model\nFitzHugh–Nagumo model\nMorris–Lecar model\nHindmarsh–Rose model\nIzhikevich model\nCable theory\n\nmodels with a threshold which generate a spike for a given threshold\n\nPerfect Integrate-and-fire\nLeaky Integrate-and-fire\nAdaptive Integrate-and-fire"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Benjamin Bourbon",
    "section": "",
    "text": "I’m a young engineer with a specialization in High Performance Computing and Artificial Intelligence.\nI develop currently a CAD library pymadcad with a friend.\nThis blog aims to share some personal work which I find useful."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "In progress",
    "section": "",
    "text": "Introduction to Spiking Neural Networks\n\n\n\n\n\n\n\nartificial intelligence\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2023\n\n\nBenjamin Bourbon\n\n\n\n\n\n\n  \n\n\n\n\nSpur Tooth Profile\n\n\n\n\n\n\n\nmechanic\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2023\n\n\nBenjamin Bourbon\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/introduction-spiking-nn/index.html#case-where-i-ne-0",
    "href": "posts/introduction-spiking-nn/index.html#case-where-i-ne-0",
    "title": "Introduction to Spiking Neural Networks",
    "section": "Case where \\(I \\ne 0\\)",
    "text": "Case where \\(I \\ne 0\\)\n\n\n\nFigure : RC circuit\n\n\nWe apply the Kirchhoff’s current law on the green point : \\[\nI_e = I_{s_1} + I_{s_2}\n\\] We use the characteristic relation of a resistor (Ohm’s law): \\[\nU = RI\n\\]\nAnd the characteristic relation of a capacitor : \\[\nI = C \\times \\frac {dU}{dt}\n\\]\nWe represent the intensity as \\(I(t)\\) and the tension as \\(V_m(t)\\). Thus we get the following relation: \\[\nI(t) = \\frac{V_m(t)}{R_m} + C_m \\frac{dV(m)}{dt}\n\\] \\[\n\\boxed{C_m \\frac{dV(m)}{dt} = I(t) - \\frac{V_m(t)}{R_m}}\n\\]"
  },
  {
    "objectID": "posts/introduction-spiking-nn/index.html#resolution-of-the-differential-equation",
    "href": "posts/introduction-spiking-nn/index.html#resolution-of-the-differential-equation",
    "title": "Introduction to Spiking Neural Networks",
    "section": "Resolution of the differential equation",
    "text": "Resolution of the differential equation\n\nHomogeneous solution\n\\[\n\\frac{dV(m)}{dt} + \\frac{V_m(t)}{R_m C_m} = 0\n\\]\nWe set \\(\\tau = R_m C_m\\). Then we get: \\[\nV_m(t) = A e^{\\frac{-t}{\\tau}}\n\\] where \\(A\\) is the integration constant of the problem.\n\nParticular solution\nWe assume that \\(I(t)\\) is constant. The particular solution has the same form of the second member of the equation. In other words, \\(\\frac{dV(m)}{dt} = 0\\).\n\\[\n\\implies V_m(t) = R_m I(t)\n\\]\n\n\nGeneral solution\n\\[\nV_m(t) = A e^{\\frac{-t}{\\tau}} + R_m I(t)\n\\]\nWe assume that \\(V_m(0^+) = V_m(0^-) = 0\\). Thus, we get: \\[\nA + R_m I = 0 \\iff A = - R_m I\n\\]\nSo:\n\\[\n\\boxed{V_m(t) = R_m I(1 - e^{\\frac{-t}{\\tau}})}\n\\]"
  },
  {
    "objectID": "posts/introduction-spiking-nn/index.html#calculation-of-the-fire-frequency",
    "href": "posts/introduction-spiking-nn/index.html#calculation-of-the-fire-frequency",
    "title": "Introduction to Spiking Neural Networks",
    "section": "Calculation of the fire frequency",
    "text": "Calculation of the fire frequency\nThe model can become more precise by introducing a refractory time in which the neuron can not be discharged. We are interesting to evaluate the frequency when \\(I > I_{th}\\) (when \\(V_m(t)\\) is constant, \\(I_{th} = \\frac{V_{th}}{R_m}\\)): \\[\n\\begin{align}\n         & V_{th} = R_m I(1 - e^{\\frac{-(t - t_{ref})}{\\tau}}) \\\\\n    \\iff & e^{\\frac{-(t - t_{ref})}{\\tau}} = 1 - \\frac{V_{th}}{I R_m} \\\\\n    \\iff & \\frac{-(t - t_{ref})}{\\tau} = \\log{(1 - \\frac{V_{th}}{I R_m})} \\\\\n    \\iff & t = t_{ref} - \\tau \\log{(1 - \\frac{V_{th}}{I R_m})}\n\\end{align}\n\\]\nThen, we can define the fire frequency with the inverse of the total gap between the impulses (including the down-time). The fire frequency is then :\n\\[\n\\boxed{\n    f(I) = \\left\\{\\begin{array}{ll}\n    0, & I \\le I_{th} \\\\\n    \\left[t_{ref} - \\tau \\log{\\left(1 - \\frac{V_{th}}{I R_m}\\right)}\\right]^{-1}, & I > I_{th}\n    \\end{array}\\right.\n}\n\\]\n\nCase where \\(I = 0\\)\n\n\n\nFigure : RC Circuit\n\n\nWe apply the Kirchhoff’s current law on the green point : \\[\nI_{s_1} + I_{s_2} = 0\n\\]\nWe get the following relation : \\[\n\\frac{V_m(t)}{R_m} + C_m \\frac{dV(m)}{dt} = 0\n\\] \\[\n\\boxed{\\frac{dV(m)}{dt} + \\frac{V_m(t)}{\\tau} = 0}\n\\]\nwith \\(\\tau = R_m C_m\\)\n\n\nSolution of the differential equation\n\\[\nV_m(t) = A e^{\\frac{-t}{\\tau}}\n\\]\nwhere \\(A\\) is the integration constant of the problem.\nWe assume \\(V_m(0) = I R_m\\). Thus : \\[\nA = I R_m\n\\]\nWe get then :\n\\[\n\\boxed{V_m(t) = I R_m e^{\\frac{-t}{\\tau}}}\n\\]"
  },
  {
    "objectID": "posts/introduction-spiking-nn/index.html#conclusion",
    "href": "posts/introduction-spiking-nn/index.html#conclusion",
    "title": "Introduction to Spiking Neural Networks",
    "section": "Conclusion",
    "text": "Conclusion\nWe recall that \\(\\tau\\) is characterized of the duration which makes the start-up level to disappear to be replaced by the permanent permanent. The permanent level is reached after several \\(\\tau\\) (\\(\\approx 5 \\tau\\)). We can do the analogy \\(V_m\\), the tension of bounds of the cellular membrane and \\(R_m\\) the membrane resistance.\n\n\n\nImpulse\n\n\nBenefits - the model will not keep an increase of the tension for ever contrary to other models without leak where it is kept until the appearance of a new impulse.\nDrawbacks - the model does not take into account the neuronal adaptation, so that it can not describe spike series."
  },
  {
    "objectID": "posts/introduction-spiking-nn/index.html#architecture-of-a-snn",
    "href": "posts/introduction-spiking-nn/index.html#architecture-of-a-snn",
    "title": "Introduction to Spiking Neural Networks",
    "section": "Architecture of a SNN",
    "text": "Architecture of a SNN\nEven if SNN have an unique concept, they stay a neural network. We can find :\n\nFeedforward Neural Network\nRecurrent Neural Network\nSynfire chain\nReservoir computing"
  },
  {
    "objectID": "posts/introduction-spiking-nn/index.html#spike-timing-dependent-plasticity-stdp",
    "href": "posts/introduction-spiking-nn/index.html#spike-timing-dependent-plasticity-stdp",
    "title": "Introduction to Spiking Neural Networks",
    "section": "Spike-Timing-Dependent Plasticity (STDP)",
    "text": "Spike-Timing-Dependent Plasticity (STDP)\nIt is a unsupervised learning mechanism. The training is realized layer by layer, in other words, the training of the current layer is made when the training of the previous layer is finished. Neurons of the layer compete with each other and those which fire quickly, trigger a STDP and learn from inputs:\n\\[\n\\Delta \\omega_{ij} =\n\\left\\{\n\\begin{array}{c}\na^+ \\omega_{ij}(1 - \\omega_{ij}), & si & t_j - t_i \\le 0 \\\\\na^- \\omega_{ij}(1 - \\omega_{ij}), & si & t_j - t_i > 0\n\\end{array}\n\\right.\n\\] where \\(i\\) and \\(j\\) respectively refer at the index of postsynaptic neurons and presynaptic neurons, \\(t_i\\) and t_j$ are the time related of spikes, \\(\\Delta \\omega_{ij}\\) is the modification of synaptic weights and \\(a^+\\) and \\(a^-\\) are two specific learning rates.\nWe measure the learning convergence of the \\(l\\)-nth layer under the shape :\n\\[\nC_l = \\sum_f \\sum_i \\frac{\\omega_{f,i}(1 - \\omega_{f,i})}{n_\\omega}\n\\]\nwhere \\(C_l\\) tends to \\(0\\) if each of synaptic weights converge to \\(0\\) or \\(1\\). Therefore, we stop the \\(l\\)-nth layer learning when \\(C_l\\) is enough close to \\(0\\) (i.e. \\(C_l < 0.01\\))."
  },
  {
    "objectID": "posts/introduction-spiking-nn/index.html#calculation-of-the-firing-frequency",
    "href": "posts/introduction-spiking-nn/index.html#calculation-of-the-firing-frequency",
    "title": "Introduction to Spiking Neural Networks",
    "section": "Calculation of the firing frequency",
    "text": "Calculation of the firing frequency\nThe model can become more precise by introducing a refractory time in which the neuron can not be discharged. We are interesting to evaluate the frequency when \\(I > I_{th}\\) (when \\(V_m(t)\\) is constant, \\(I_{th} = \\frac{V_{th}}{R_m}\\)): \\[\n\\begin{align}\n         & V_{th} = R_m I(1 - e^{\\frac{-(t - t_{ref})}{\\tau}}) \\\\\n    \\iff & e^{\\frac{-(t - t_{ref})}{\\tau}} = 1 - \\frac{V_{th}}{I R_m} \\\\\n    \\iff & \\frac{-(t - t_{ref})}{\\tau} = \\log{(1 - \\frac{V_{th}}{I R_m})} \\\\\n    \\iff & t = t_{ref} - \\tau \\log{(1 - \\frac{V_{th}}{I R_m})}\n\\end{align}\n\\]\nThen, we can define the firing frequency with the inverse of the total gap between the impulses (including the down-time). The firing frequency is then :\n\\[\n\\boxed{\n    f(I) = \\left\\{\\begin{array}{ll}\n    0, & I \\le I_{th} \\\\\n    \\left[t_{ref} - \\tau \\log{\\left(1 - \\frac{V_{th}}{I R_m}\\right)}\\right]^{-1}, & I > I_{th}\n    \\end{array}\\right.\n}\n\\]\n\nCase where \\(I = 0\\)\n\n\n\nFigure : RC Circuit\n\n\nWe apply the Kirchhoff’s current law on the green point : \\[\nI_{s_1} + I_{s_2} = 0\n\\]\nWe get the following relation : \\[\n\\frac{V_m(t)}{R_m} + C_m \\frac{dV(m)}{dt} = 0\n\\] \\[\n\\boxed{\\frac{dV(m)}{dt} + \\frac{V_m(t)}{\\tau} = 0}\n\\]\nwith \\(\\tau = R_m C_m\\)\n\n\nSolution of the differential equation\n\\[\nV_m(t) = A e^{\\frac{-t}{\\tau}}\n\\]\nwhere \\(A\\) is the integration constant of the problem.\nWe assume \\(V_m(0) = I R_m\\). Thus : \\[\nA = I R_m\n\\]\nWe get then :\n\\[\n\\boxed{V_m(t) = I R_m e^{\\frac{-t}{\\tau}}}\n\\]"
  }
]